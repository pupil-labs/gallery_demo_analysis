{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ba2f0e",
   "metadata": {},
   "source": [
    "# PupilLabs Reference Image Mapper for AOIs in dynamic scenes\n",
    "\n",
    "The biggest difference when using the Reference Image Mapper for dynamic scenes is that we will now need multiple reference images. Each reference image now represents one individual painting, so there is no more difference between the AOI and the reference images.\n",
    "\n",
    "The gaze data is still stored in one folder per reference image - so we will end up having as many data folders as we have AOIs to analyse. While this sounds like we might have more information here that we had in the static case, we actually loose something important: the reference images don't reveal anything about their location in the art gallery. We won't know from looking at them how they were positioned relative to each other. \n",
    "\n",
    "Still, we can compute metrics that are comparable, or even equal to the metrics in the static case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b570609",
   "metadata": {},
   "source": [
    "## Step 1: Load the data\n",
    "\n",
    "Because we need to load the data from a bunch of different sub-folders, we create a function that loops through the files for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdad794",
   "metadata": {},
   "source": [
    "## Step 1: Load the data\n",
    "\n",
    "Because we need to load the data from a bunch of different sub-folders, we create a function that loops through the files for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d2c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_path = './reference_export/walking/'\n",
    "\n",
    "def get_file_list(file_name, directory = dynamic_path):\n",
    "    all_aoi_recordings = os.listdir(directory)\n",
    "    return [f\"{directory}{aoi}/{file_name}\" for aoi in all_aoi_recordings]\n",
    "\n",
    "def load_csv_files(file_name):\n",
    "    \n",
    "    if not '.csv' in file_name:\n",
    "        file_name = file_name + '.csv'\n",
    "        \n",
    "    file_list = get_file_list(file_name)\n",
    "    section_list = get_file_list('sections.csv')\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for file, section in zip(file_list, section_list):\n",
    "        \n",
    "        df = pd.read_csv(file)\n",
    "        sections = pd.read_csv(section)\n",
    "        df['recording name'] = df['recording id'].replace(sections['recording id'].values, sections['recording name'].values)\n",
    "        df['AOI'] = file.split('MAPPER_')[1].split('_walking')[0]\n",
    "        \n",
    "        data = pd.concat([data,df], ignore_index = True)\n",
    "        \n",
    "    return data\n",
    "\n",
    "dynamic_sections = load_csv_files('sections.csv')\n",
    "dynamic_gaze = load_csv_files('gaze.csv')\n",
    "dynamic_fixations = load_csv_files('fixations.csv')\n",
    "dynamic_fixations['AOI'] = dynamic_fixations['AOI'].replace({\n",
    "    'Adel_Dauood_2': 'AD 2',\n",
    "    'Adel_Dauood_3': 'AD 3',\n",
    "    'Adel_Dauood_4': 'AD 4',\n",
    "    'Christopher_Blanc_2': 'Blanc 2',\n",
    "    'Gary_Cain' : 'Cain',\n",
    "    'Gunlief_Grube': 'Grube',\n",
    "    'Ingerlise_Vikne': 'Vikne',\n",
    "    'Vivian_Hoi_Nielsen': 'Nielsen'\n",
    "    \n",
    "})\n",
    "\n",
    "dynamic_images = get_file_list('reference_image.jpeg')\n",
    "# observer ids\n",
    "dynamic_observers = np.unique(dynamic_gaze['recording name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496e0b3",
   "metadata": {},
   "source": [
    "#### Look at the reference images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60dc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot for the images\n",
    "fig_reference, ax_reference = plt.subplots(int(np.ceil(len(dynamic_images)/4)),4, figsize = (25,12))\n",
    "\n",
    "for ax, im in zip(ax_reference.flatten(),dynamic_images):\n",
    "    dynamic_reference_image = Image.open(im)\n",
    "    ax.imshow(np.asarray(dynamic_reference_image))\n",
    "    ax.set_title(im.split('MAPPER_')[1].split('_walking')[0])\n",
    "    \n",
    "plt.savefig('./images/multiple_reference_images.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9519f9",
   "metadata": {},
   "source": [
    "You can see how each reference image shows one painting. That means, we can skip the steps in which we define the AOIs - they are defined by the identity of the reference image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d004c",
   "metadata": {},
   "source": [
    "## Step 2: Filter the data for gaze/fixation on a reference image\n",
    "\n",
    "The eye tracker keeps recording even when none of the paintings of interest is in sight. This results in many gaze and fixations instances that are not detected on an reference image, but somewhere in between. \n",
    "\n",
    "Before we want to analyse the data, we have to remove gaze samples outside of the reference imagess. The column 'gaze detected on refernence image' gives us a handy way to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The gaze data frame contained originally {len(dynamic_gaze)} samples.\")\n",
    "print(f\"The fixation data frame contained originally {len(dynamic_fixations)} samples.\")\n",
    "\n",
    "# filter the data frames \n",
    "dynamic_gaze = dynamic_gaze[dynamic_gaze['gaze detected in reference image']] \n",
    "dynamic_fixations_unfiltered = dynamic_fixations.copy()\n",
    "dynamic_fixations = dynamic_fixations[dynamic_fixations['fixation detected in reference image']] \n",
    "\n",
    "print(f\"After cleaning, the gaze data frame contained {len(dynamic_gaze)} samples.\")\n",
    "print(f\"After cleaning, the fixation data frame contained {len(dynamic_fixations)} samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecb9e6",
   "metadata": {},
   "source": [
    "## Step 3: Visual Journey\n",
    "Showing scanpaths between the images is not possible anymore. \n",
    "\n",
    "So instead of visualising how the eyes travel from image to image in space, we will show how they travel from image to image in time, using an approach we call **visual journey**. \n",
    "In the visual journey, we visualize the times when fixations were detected on any of the reference images. \n",
    "\n",
    "Since our participants all started their walk in the museum at different times, we will align the data to the start of the section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31927733",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_fixations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_timestamps(df, time_column, observers = dynamic_observers):\n",
    "    for observer in observers:\n",
    "        obs_idx = df[df['recording name'] == observer].index\n",
    "        section_start_time = min(dynamic_sections[dynamic_sections['recording name'] == observer]['section start time [ns]'].values)\n",
    "        df.loc[obs_idx, 'aligned timestamp [s]'] = (df.loc[obs_idx, time_column] - section_start_time)/1e9 \n",
    "        \n",
    "    return df['aligned timestamp [s]']\n",
    "\n",
    "dynamic_gaze['aligned timestamp [s]'] = align_timestamps(dynamic_gaze, 'timestamp [ns]')\n",
    "\n",
    "# next, we can plot the visual journey\n",
    "sns.catplot(\n",
    "    data=dynamic_gaze,\n",
    "    x=\"aligned timestamp [s]\",\n",
    "    y=\"recording name\",\n",
    "    hue=\"AOI\",\n",
    "    aspect=1,\n",
    "    linewidth=0,\n",
    "    s=1,\n",
    "    palette = 'YlGnBu',\n",
    "    height = 7\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30d3ba",
   "metadata": {},
   "source": [
    "## Step 4: Metrics\n",
    "We can compute some metrics on this data, too.\n",
    "\n",
    "### Reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_fixation_counts = dynamic_fixations.pivot_table(index='recording name', \n",
    "                      columns = 'AOI',\n",
    "                      values='fixation id',\n",
    "                      fill_value=0, \n",
    "                      aggfunc='count').unstack().to_frame().rename(columns={0:'fixation count'})\n",
    "dynamic_fixation_counts.reset_index(inplace = True)\n",
    "\n",
    "# We can generalize this plot by showing what proportion of subjects had at least on fixation of this image\n",
    "dynamic_hits = dynamic_fixation_counts.copy()\n",
    "dynamic_hits.loc[:,'hit'] = dynamic_hits['fixation count']>0\n",
    "\n",
    "dynamic_reach = dynamic_hits.groupby('AOI').mean().reset_index()\n",
    "dynamic_reach.loc[:,'hit'] =  dynamic_reach['hit']*100\n",
    "\n",
    "sns.catplot(x=\"AOI\", y=\"hit\", kind=\"bar\", data=dynamic_reach, palette = \"Greens\")\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "\n",
    "plt.subplots_adjust(bottom=0.21)\n",
    "plt.savefig('./images/hit_rate_dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ff711",
   "metadata": {},
   "outputs": [],
   "source": [
    "Each of our defined AOIs was looked at by all observers in the study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1944f9",
   "metadata": {},
   "source": [
    "### Time to first contact\n",
    "Computing the time to first contact is a bit unfair in this case. The rooms were visited in a specific order that forces a temporal pattern on the order in which the imaged are seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ecd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_fixations['aligned timestamp [s]'] = align_timestamps(dynamic_fixations, 'start timestamp [ns]')\n",
    "\n",
    "dynamic_first_contact = pd.DataFrame(dynamic_fixations.groupby(['recording name', 'AOI']).min()['aligned timestamp [s]'])\n",
    "dynamic_first_contact.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can compute the mean time to first contact\n",
    "sns.catplot(x=\"AOI\", y=\"aligned timestamp [s]\", kind=\"bar\", data=dynamic_first_contact, palette = \"Greens\", ci = 95);\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "plt.subplots_adjust(bottom=0.21)\n",
    "plt.savefig('./images/first_contact_dynamic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a85057",
   "metadata": {},
   "source": [
    "### Dwell time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a486489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add recording name for better readablity\n",
    "dynamic_dwell = dynamic_fixations.groupby(['recording name', 'AOI']).sum()['duration [ms]']\n",
    "dynamic_dwell = pd.DataFrame(dynamic_dwell)\n",
    "dynamic_dwell.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d67178",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"AOI\", y=\"duration [ms]\", kind=\"bar\", data=dynamic_dwell, palette = \"Greens\", ci = 95)\n",
    "\n",
    "plt.xticks(rotation = 90)\n",
    "plt.subplots_adjust(bottom=0.21)\n",
    "plt.savefig('./images/dwell-time_dynamic.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaze on Blanc 2\n",
    "blanc_fixations = dynamic_fixations[dynamic_fixations['AOI'] == 'Blanc 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "blanc_example_fixations = blanc_fixations[blanc_fixations['recording name']== 'JR_U_W']\n",
    "\n",
    "\n",
    "blanc_fig, blanc_axs = plt.subplots(1,1, figsize = (10,7))\n",
    "blanc_axs.imshow(np.asarray(dynamic_reference_image))\n",
    "#blanc_axs.scatter(blanc_example_fixations['fixation x [px]'],\n",
    "#                     blanc_example_fixations['fixation y [px]'],facecolor='none', edgecolor='cyan', \n",
    "#                     linewidth=2, s = blanc_example_fixations['duration [ms]']);\n",
    "#blanc_axs.plot(blanc_example_fixations['fixation x [px]'], blanc_example_fixations['fixation y [px]'], \n",
    "#                  color='grey');\n",
    "\n",
    "plt.savefig('./blanc_clean.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d1144",
   "metadata": {},
   "source": [
    "Meeting Marc and Richard\n",
    "\n",
    "- AOI size\n",
    "- where\n",
    "\n",
    "\n",
    "- who\n",
    "\n",
    "\n",
    "- what\n",
    "case study - research question (perception in a gallery)\n",
    "vs areas of interest with the reference image mapper\n",
    "content roadmap\n",
    "pull out how-tos, methods from notebook\n",
    "\n",
    "cloud tutorial session -> what's there now?\n",
    "make your first enrichment\n",
    "stuff on a shelf\n",
    "supermarket was never released - linked to individual, but not \n",
    "what in this thing generalizes to a lot of stuff? \n",
    "\n",
    "\n",
    "- How-To steps \n",
    "csv data - enrichment "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
